# -*- coding: utf-8 -*-
"""Phase 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Eu0gAUQXdKhxC3lk65v5baO0FRsbcZl0

Importing Libraries and functions
"""

import torch
!pip install -q segmentation-models-pytorch
import segmentation_models_pytorch as smp
import segmentation_models_pytorch.utils
import torch.nn as nn
import torchvision
from torchvision import datasets, transforms, models
from torch.utils.data import Dataset
import random
import torchvision.transforms.functional as TF

import shutil
import matplotlib.pyplot as plt
import os

import PIL
from PIL import Image
import cv2
import numpy
import numpy as np

from scipy.ndimage import distance_transform_edt

shutil.rmtree("/content/drive/MyDrive/Final/png")

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(device)

from google.colab import drive
drive.mount('/content/drive')



"""# Single cell"""

!unzip '/content/drive/MyDrive/Final Project/New database pictures.zip'

!mkdir images

!mkdir masks

l=['carcinoma_in_situ','light_dysplastic','moderate_dysplastic','normal_columnar','normal_intermediate','normal_superficiel','severe_dysplastic']
for j in range(len(l)):

  path = '/content/New database pictures/'+l[j]


  for img in os.listdir(path):
    i= path+'/'+ img
    im=Image.open(i)
    ip='/content/images/'+img
    mp='/content/masks/'+img
    n= len(i)
    if (i[n-5]!='d'):
      im.save(ip)
    else:
      mask = cv2.imread(i)
      red = [0, 0, 255]
      gray = [128, 128, 128]
      blue = [255, 0, 0]
      cyt = [128,0,0]

      # Create the masks for each color
      nucleus_mask = cv2.inRange(mask, np.array(blue), np.array(blue))
     # cv2_imshow(nucleus_mask)
      cytoplasm_mask = cv2.inRange(mask, np.array(cyt), np.array(cyt))
      background_mask = cv2.bitwise_or(cv2.inRange(mask, np.array(gray), np.array(gray)), cv2.inRange(mask, np.array(red), np.array(red)))

      # Merge the masks into a single array
      mask_array = np.dstack((background_mask, cytoplasm_mask, nucleus_mask))
      cv2.imwrite(mp, mask_array)

path1='/content/images'
path2='/content/masks'


p1 = sorted(os.listdir(path1))
p2 = sorted(os.listdir(path2))
#p3 = sorted(os.listdir(pathm))

print(len(p1))
print(len(p2))
#print(len(p3))

class Dataset(Dataset):

    def __init__(self, images_dir, masks_dir, augmentation = None,preprocessing = None):

        self.ids = sorted(os.listdir(images_dir))
        self.idss = sorted(os.listdir(masks_dir))
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.idss]
        self.augmentation = augmentation
        self.preprocessing = preprocessing

    def transform(self, image, mask):

        # # To PilImage
        # tp = transforms.ToPILImage(mode=None)
        # image = tp(image)
        # mask = tp(mask)

        # Resize
        resize = transforms.Resize((256,256))
        image = resize(image)
        mask = resize(mask)


        # Random horizontal flipping
        if random.random() > 0.5:
            image = TF.hflip(image)
            mask = TF.hflip(mask)

        # Random vertical flipping
        if random.random() > 0.5:
            image = TF.vflip(image)
            mask = TF.vflip(mask)

        image = numpy.array(image)
        mask = numpy.array(mask)

        return image, mask


    def __getitem__(self, i):


        # read data
        # image = cv2.imread(self.images_fps[i])
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # mask = cv2.imread(self.masks_fps[i])
        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)

        image = Image.open(self.images_fps[i]).convert('RGB')
        mask = Image.open(self.masks_fps[i]).convert('RGB')

        # apply augmentations
        if self.augmentation:
            image,mask = self.transform(image=image, mask=mask)
            # sample = self.augmentation(image=image, mask=mask)
            # image, mask = sample['image'], sample['mask']

        if self.preprocessing:
          sample = self.preprocessing(image=image)
          image = sample['image']
          mask = TF.to_tensor(mask)
        else:
          image = TF.to_tensor(image)
          mask = TF.to_tensor(mask)



        return image, mask

    def __len__(self):
        return len(self.ids)

dt = Dataset(path1,path2,augmentation = True)

train_set_single,val_set_single = torch.utils.data.random_split(dt,[417,500])
val_set_single,test_set_single = torch.utils.data.random_split(val_set_single,[250,250])

data_dict = {'train' : torch.utils.data.DataLoader(dataset = train_set_single, batch_size = 4, shuffle =True),
'val' : torch.utils.data.DataLoader(dataset = val_set_single, batch_size = 4, shuffle = True)}



dataiter = iter(data_dict['train'])
images,masks = next(dataiter)

print(images.shape)
print(images[1].shape)

"""##Image Visualization


"""

def imshow(img):
    npimg = img.numpy()
    plt.figure(figsize=(12, 4))
    plt.axis('off')
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    # plt.title(title)

def show_batch_images(dataloader):
    images,labels= next(iter(dataloader))
    img = torchvision.utils.make_grid(images)
    msk = torchvision.utils.make_grid(labels)
    imshow(img)
    imshow(msk)

for i in range(6):
    show_batch_images(data_dict['train'])



"""# Multicell"""

!unzip '/content/drive/MyDrive/Amrita Dessertation/Phase -2/png.zip'

"""###Color code"""

from google.colab.patches import cv2_imshow

path1='/content/png/00000.png'
i= path1
im=Image.open(i)
ip='/content/images/'
mp='/content/masks/'
n= len(i)
image = cv2.imread(i)
image=cv2.resize(image,(256,256))
n_p1 =[130,59,122]
n_p2=[185,128,193]
#nucleus_pink = [ 61,21,110]
n_b1=[66,120,147]
n_b2=[120,143,176]
#nucleus_blue =[23,45,118]
b1=[178,169,196]
b2=[100,167,194]
#back= [195,185,220]
cyt_pink=[179,113,174]
cyt_blue=[94,134,170]

nucleus_pink_mask = cv2.inRange(image, np.array(n_p1), np.array(n_p2))
nucleus_blue_mask = cv2.inRange(image, np.array(n_b2), np.array(n_b1))
back_mask = cv2.inRange(image, np.array(b2), np.array(b1))
cyt_pink_mask = cv2.inRange(image, np.array(cyt_pink), np.array(cyt_pink))
cyt_blue_mask = cv2.inRange(image, np.array(cyt_blue), np.array(cyt_blue))

# Merge the masks into a single array
nucleus_mask = cv2.bitwise_or(nucleus_pink_mask, nucleus_blue_mask)
cytoplasm_mask = cv2.bitwise_or(cyt_pink_mask, cyt_blue_mask)
background_mask = back_mask

mask_array = np.dstack((background_mask, cytoplasm_mask, nucleus_mask))
'''

nucleus_pink_lower = np.array(nucleus_pink, dtype=np.uint8)
nucleus_pink_upper = np.array(nucleus_pink, dtype=np.uint8)
nucleus_blue_lower = np.array(nucleus_blue, dtype=np.uint8)
nucleus_blue_upper = np.array(nucleus_blue, dtype=np.uint8)
back_lower = np.array(back, dtype=np.uint8)
back_upper = np.array(back, dtype=np.uint8)
cyt_pink_lower = np.array(cyt_pink, dtype=np.uint8)
cyt_pink_upper = np.array(cyt_pink, dtype=np.uint8)
cyt_blue_lower = np.array(cyt_blue, dtype=np.uint8)
cyt_blue_upper = np.array(cyt_blue, dtype=np.uint8)

# Create masks for each region
nucleus_pink_mask = cv2.inRange(image, nucleus_pink_lower, nucleus_pink_upper)
nucleus_blue_mask = cv2.inRange(image, nucleus_blue_lower, nucleus_blue_upper)
back_mask = cv2.inRange(image, back_lower, back_upper)
cyt_pink_mask = cv2.inRange(image, cyt_pink_lower, cyt_pink_upper)
cyt_blue_mask = cv2.inRange(image, cyt_blue_lower, cyt_blue_upper)

# Merge the masks into a single array
nucleus_mask = cv2.bitwise_or(nucleus_pink_mask, nucleus_blue_mask)
cytoplasm_mask = cv2.bitwise_or(cyt_pink_mask, cyt_blue_mask)
background_mask = back_mask

# Set the color values for each region in the output image
output_image = np.zeros_like(image)
output_image[nucleus_mask > 0] = (0, 0, 255)  # Red for nucleus
output_image[background_mask > 0] = (255, 0, 0)  # Blue for background
output_image[cytoplasm_mask > 0] = (0, 255, 0)  # Green for cytoplasm

    # Show the output image (optional)
'''
cv2_imshow(image)
cv2_imshow(nucleus_pink_mask)
cv2_imshow(nucleus_blue_mask)
cv2_imshow(back_mask)
cv2_imshow(mask_array)

path = '/content/png'
for img in os.listdir(path):
    i= path+'/'+ img
    im=Image.open(i)
    ip='/content/images/'+img
    mp='/content/masks/'+img
    n= len(i)
    image = cv2.imread(i)
    nucleus_pink = [ 61,21,110]
    nucleus_blue =[23,45,118]
    back= [195,185,220]
    cyt_pink=[179,113,174]
    cyt_blue=[94,134,170]

# Define color ranges for each region
    nucleus_pink_range = np.array([nucleus_pink, nucleus_pink], dtype=np.uint8)
    nucleus_blue_range = np.array([nucleus_blue, nucleus_blue], dtype=np.uint8)
    back_range = np.array([back, back], dtype=np.uint8)
    cyt_pink_range = np.array([cyt_pink, cyt_pink], dtype=np.uint8)
    cyt_blue_range = np.array([cyt_blue, cyt_blue], dtype=np.uint8)

# Create masks for each region
    nucleus_pink_mask = cv2.inRange(image, nucleus_pink_range[0], nucleus_pink_range[1])
    nucleus_blue_mask = cv2.inRange(image, nucleus_blue_range[0], nucleus_blue_range[1])
    back_mask = cv2.inRange(image, back_range[0], back_range[1])
    cyt_pink_mask = cv2.inRange(image, cyt_pink_range[0], cyt_pink_range[1])
    cyt_blue_mask = cv2.inRange(image, cyt_blue_range[0], cyt_blue_range[1])

    # Combine masks to create the final mask
    nucleus_mask = cv2.bitwise_or(nucleus_pink_mask, nucleus_blue_mask)
    background_mask = back_mask
    cytoplasm_mask = cv2.bitwise_or(cyt_pink_mask, cyt_blue_mask)

    # Set the color values for each region in the output image
    output_image = np.zeros_like(image)
    output_image[nucleus_mask > 0] = (0, 0, 255)  # Red for nucleus
    output_image[background_mask > 0] = (255, 0, 0)  # Blue for background
    output_image[cytoplasm_mask > 0] = (0, 255, 0)  # Green for cytoplasm

    # Show the output image (optional)
    cv2_imshow(output_image)
    #cv2.waitKey(0)
    #cv2.destroyAllWindows()

    # Save the output image
   # cv2.imwrite('output_image.png', output_image)




   # cv2.imwrite(mp, mask_array)

"""###Augmentation and dataset"""



pathm= '/content/png'
path1= '/content/png'

p1 = sorted(os.listdir(pathm))

im = mpimg.imread('/content/png/00000.png')

import matplotlib.pyplot as plt
import matplotlib.image as mpimg
from torchvision import transforms

# Load the image
im = mpimg.imread('/content/png/00000.png')

# Convert image to PIL format
to_pil = transforms.ToPILImage(mode='RGB')
pil_image = to_pil(im)

# Apply ColorJitter transformation to the PIL image
cj = transforms.ColorJitter(brightness=0.01, contrast=10, saturation=0, hue=0)
im1 = TF.adjust_sharpness(pil_image,sharpness_factor=1)

# Display the transformed image
plt.imshow(im1)
plt.axis('off')
plt.show()



import matplotlib.image as mpimg

cj = transforms.ColorJitter(brightness=0.8, contrast=0, saturation=0, hue=0)
cj1 = transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0.3, hue=0)
cj2 = transforms.ColorJitter(brightness=0.6, contrast=0, saturation=0, hue=0)

cj3 = transforms.ColorJitter(brightness=0.5, contrast=0, saturation=0, hue=0.5)
cj4 = transforms.ColorJitter(brightness=0.8, contrast=0, saturation=0.3, hue=0.3)
cj5 = transforms.ColorJitter(brightness=0.3, contrast=0.2, saturation=0, hue=0.3)
cj6 = transforms.ColorJitter(brightness=0.8, contrast=0.2, saturation=0, hue=0.3)
cj7 = transforms.ColorJitter(brightness=0.7, contrast=0.1, saturation=0, hue=0.3)
cj8 = transforms.ColorJitter(brightness=0.8, contrast=0.2, saturation=0.4, hue=0.3)

rs = transforms.RandomAdjustSharpness(1.3, p=0.5)



for img in (p1):

  img_path = pathm+'/'+img # Making image file path
  n = len(img)
  imp = img[:(n-4)]
  print(imp)



  im = mpimg.imread(img_path)
  #print(im)
  #To PilImage

  im = transforms.ToPILImage(mode=None)(im.astype('uint8'))

  im_rescaled = (im - np.min(im)) / (np.max(im) - np.min(im)) * 255

# Convert image to PIL format
  to_pil = transforms.ToPILImage(mode='RGB')
  im = to_pil(im_rescaled.astype('uint8'))

  im1 = TF.hflip(im)
  fp =  path1 + '/' + imp + 't1' +  '.png'
  im1.save(fp)



  im2 = TF.vflip(im)
  fp = path1 + '/' + imp + 't2' +  '.png'
  im2.save(fp)



  im4 =TF.adjust_sharpness(im,sharpness_factor=1)
  fp = path1 + '/' + imp + 't3' +  '.png'
  im4.save(fp)



  im5 = cj3(im)
  fp = path1 + '/' + imp + 't4' +  '.png'
  im5.save(fp)

  im9 = rs(im)
  fp = path1 + '/' + imp + 't8' +  '.png'
  im9.save(fp)

  im11 = TF.equalize(im)
  fp = path1 + '/' + imp + 't10' +  '.png'
  im11.save(fp)



p1 = sorted(os.listdir(pathm))
len(p1)

class MulticellDataset(Dataset):

    def __init__(self, images_dir, augmentation=None, preprocessing=None,img_trans=None):
        self.ids = sorted(os.listdir(images_dir))
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.augmentation = augmentation
        self.preprocessing = preprocessing
        self.img_trans=img_trans

    def transform(self, image):
        resize = transforms.Resize((256, 256))
        image = resize(image)
       # image = image.convert("L")

        if random.random() > 0.5:
            image = TF.hflip(image)

        if random.random() > 0.5:
            image = TF.vflip(image)

        image = numpy.array(image)

        return image
    def data_transforms(self, image):
        seed = np.random.randint(2147483647)

    # Apply horizontal flip with 50% probability
        if random.random() < 0.5:
          image = TF.hflip(image)


    # Apply vertical flip with 50% probability
        if random.random() < 0.5:
          image = TF.vflip(image)

        '''
        if random.random()<0.5:
          image= TF.rotate(image,angle=2)


        if random.random()<0.5:
          image = TF.rotate(image,angle=3)
        '''

        if random.random()<0.5:
          image = TF.hflip(image)

        if random.random()<0.5:
          image=TF.vflip(image)

        if random.random()<0.5:
          image=TF.adjust_sharpness(image,sharpness_factor=2)


        if random.random()<0.5:
          image=TF.equalize(image)

        return image

    def __getitem__(self, i):
        image = Image.open(self.images_fps[i]).convert('RGB')

        if self.img_trans:
          image =self.data_transforms(image)

        if self.augmentation:
            image = self.transform(image)

        if self.preprocessing:
            sample = self.preprocessing(image=image)
            image = sample['image']
        else:
            image = TF.to_tensor(image)

        return image

    def __len__(self):
        return len(self.ids)

dm = MulticellDataset(pathm,augmentation = True,img_trans=True)

train_set_multi,val_set_multi = torch.utils.data.random_split(dm,[250,436])
val_set_multi,test_set_multi = torch.utils.data.random_split(val_set_multi,[150,286])

data_dictm = {'train_multi' : torch.utils.data.DataLoader(dataset = train_set_multi, batch_size = 4, shuffle =True),
'val_multi' : torch.utils.data.DataLoader(dataset = val_set_multi, batch_size = 4, shuffle = True)}



dataiter = iter(data_dictm['train_multi'])
images = next(dataiter)

print(images.shape)
print(images[0].shape)



"""##Image Visualization


"""

def imshow(img):
    npimg = img.numpy()
    plt.figure(figsize=(12, 4))
    plt.axis('off')
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    # plt.title(title)

def show_batch_images(dataloader):
    images= next(iter(dataloader))
    img = torchvision.utils.make_grid(images)
   # msk = torchvision.utils.make_grid(labels)
    imshow(img)
    #imshow(msk)

for i in range(6):
    show_batch_images(data_dictm['train_multi'])

"""## Multicell with masks"""



class MDataset(Dataset):

    def __init__(self, images_dir, masks_dir, augmentation = None,preprocessing = None,img_trans=None):

        self.ids = sorted(os.listdir(images_dir))
        self.idss = sorted(os.listdir(masks_dir))
        self.images_fps = [os.path.join(images_dir, image_id) for image_id in self.ids]
        self.masks_fps = [os.path.join(masks_dir, image_id) for image_id in self.idss]
        self.augmentation = augmentation
        self.preprocessing = preprocessing
        self.img_trans=img_trans

    def transform(self, image, mask):

        # # To PilImage
        # tp = transforms.ToPILImage(mode=None)
        # image = tp(image)
        # mask = tp(mask)

        # Resize
        resize = transforms.Resize((256,256))
        image = resize(image)
        mask = resize(mask)


        # Random horizontal flipping
        if random.random() > 0.5:
            image = TF.hflip(image)
            mask = TF.hflip(mask)

        # Random vertical flipping
        if random.random() > 0.5:
            image = TF.vflip(image)
            mask = TF.vflip(mask)

        image = numpy.array(image)
        mask = numpy.array(mask)

        return image, mask
    def data_transforms(self, image, mask):
        seed = np.random.randint(2147483647)

    # Apply horizontal flip with 50% probability
        if random.random() < 0.5:
          image = TF.hflip(image)
          mask = TF.hflip(mask)

    # Apply vertical flip with 50% probability
        if random.random() < 0.5:
          image = TF.vflip(image)
          mask = TF.vflip(mask)

        if random.random()<0.5:
          image= TF.rotate(image,angle=2)
          mask= TF.rotate(mask,angle=2)

        if random.random()<0.5:
          image = TF.rotate(image,angle=3)
          mask =TF.rotate(mask,angle=3)

        if random.random()<0.5:
          image = TF.hflip(image)
          mask=TF.hflip(mask)

        if random.random()<0.5:
          image=TF.vflip(image)
          mask=TF.vflip(mask)

        if random.random()<0.5:
          image=TF.adjust_sharpness(image,sharpness_factor=2)
          mask=TF.adjust_sharpness(mask,sharpness_factor=2)

        if random.random()<0.5:
          image=TF.equalize(image)
          mask=TF.equalize(mask)
        return image, mask

    def __getitem__(self, i):


        # read data
        # image = cv2.imread(self.images_fps[i])
        # image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        # mask = cv2.imread(self.masks_fps[i])
        # mask = cv2.cvtColor(mask, cv2.COLOR_BGR2RGB)

        image = Image.open(self.images_fps[i]).convert('RGB')
        mask = Image.open(self.masks_fps[i]).convert('RGB')

        if self.img_trans:
          image, mask =self.data_transforms(image,mask)
        # apply augmentations
        if self.augmentation:
            image,mask = self.transform(image=image, mask=mask)
            # sample = self.augmentation(image=image, mask=mask)
            # image, mask = sample['image'], sample['mask']

        if self.preprocessing:
          sample = self.preprocessing(image=image)
          image = sample['image']
          mask = TF.to_tensor(mask)
        else:
          image = TF.to_tensor(image)
          mask = TF.to_tensor(mask)



        return image, mask

    def __len__(self):
        return len(self.ids)

mkdir png_sub

import shutil
source_folder = '/content/png'
destination_folder = '/content/png_sub'

# Create the destination folder if it doesn't exist
if not os.path.exists(destination_folder):
    os.makedirs(destination_folder)

# Copy the first 500 images
file_list = os.listdir(source_folder)
for i, file_name in enumerate(file_list):
    if i >= 500:
        break
    shutil.copy(os.path.join(source_folder, file_name), destination_folder)

mkdir masks_sub

source_folder1 = '/content/masks'
destination_folder1 = '/content/masks_sub'

# Create the destination folder if it doesn't exist
if not os.path.exists(destination_folder1):
    os.makedirs(destination_folder1)

# Copy the first 500 images
file_list = os.listdir(source_folder1)
for i, file_name in enumerate(file_list):
    if i >= 500:
        break
    shutil.copy(os.path.join(source_folder1, file_name), destination_folder1)

path1m='/content/png_sub'
path2m='/content/masks_sub'

dt = MDataset(path1m,path2m,augmentation = True,img_trans=True)

len(dt)

train_set_single,val_set_single = torch.utils.data.random_split(dt,[250,250])
val_set_single,test_set_single = torch.utils.data.random_split(val_set_single,[150,100])

data_dicts = {'train' : torch.utils.data.DataLoader(dataset = train_set_single, batch_size = 4, shuffle =True),
'val' : torch.utils.data.DataLoader(dataset = val_set_single, batch_size = 4, shuffle = True)}

dataiter = iter(data_dicts['train'])
images,masks = next(dataiter)

print(images.shape)
print(images[1].shape)

len(data_dictm['val_multi'])

"""##Image Visualization


"""

def imshow(img):
    npimg = img.numpy()
    plt.figure(figsize=(12, 4))
    plt.axis('off')
    plt.imshow(np.transpose(npimg, (1, 2, 0)))
    # plt.title(title)

def show_batch_images(dataloader):
    images,labels= next(iter(dataloader))
    img = torchvision.utils.make_grid(images)
    msk = torchvision.utils.make_grid(labels)
    imshow(img)
    imshow(msk)

for i in range(6):
    show_batch_images(data_dicts['train'])



import torch
from torch.utils.data import DataLoader, random_split

# Assuming you have `multicell_dataset` and `mdataset` as your datasets

# Splitting the datasets
train_set_single, val_set_single, test_set_single = random_split(dm, [300, 150, 236])
train_set_labeled, val_set_labeled, test_set_labeled = random_split(dt, [300, 150, 50])

# Combining the labeled and unlabeled datasets
train_set = torch.utils.data.ConcatDataset([train_set_single, train_set_labeled])
val_set = torch.utils.data.ConcatDataset([val_set_single, val_set_labeled])
test_set = torch.utils.data.ConcatDataset([test_set_single, test_set_labeled])

# Creating data loaders
batch_size = 4
train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(dataset=val_set, batch_size=batch_size, shuffle=True)
test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=True)

data_dicts = {
    'train': train_loader,
    'val': val_loader,
    'test': test_loader
}

import matplotlib.pyplot as plt
import numpy as np

# Assuming you have created the `data_dicts` dictionary as mentioned in the previous response

# Get the training data loader
train_loader = data_dicts['train']

# Iterate over the data loader
for images, masks in train_loader:
    # Print the sizes of images and masks in each batch
    print("Batch Size - Images:", images.size(0), "Masks:", masks.size(0))

    # Check if the sizes of images and masks match within a batch
    if images.size(0) != masks.size(0):
        print("Mismatch in batch sizes of images and masks!")
        break

    # Print the shape of images and masks
    print("Shape - Images:", images.shape, "Masks:", masks.shape)
    print("Channel Size - Images:", images.size(1), "Masks:", masks.size(1))
    print("Image Size - Height:", images.size(2), "Width:", images.size(3))
    print()

    # Convert tensors to numpy arrays
    images = images.numpy()
    masks = masks.numpy()

    # Visualize a few samples
    num_samples = min(images.shape[0], 4)  # Display at most 4 samples
    for i in range(num_samples):
        image = np.transpose(images[i], (1, 2, 0))  # Transpose to (H, W, C) for visualization
        mask = np.transpose(masks[i], (1, 2, 0))  # Transpose to (H, W, C) for visualization

        # Display the image and mask
        plt.subplot(2, num_samples, i + 1)
        plt.imshow(image)
        plt.title('Image')

        plt.subplot(2, num_samples, num_samples + i + 1)
        plt.imshow(mask)
        plt.title('Mask')

    plt.tight_layout()
    plt.show()

    break  # Break after displaying the first batch

"""#Model"""



L_num=320

model=/content/drive/MyDrive/Amrita Dessertation/Saved models/checkpath_efficientNet

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
from scipy.ndimage import distance_transform_edt

# Define the Dual_Unet model
class Dual_Unet(nn.Module):
    def __init__(self, encoder_name='efficientnet-b0', encoder_weights='imagenet'):
        super(Dual_Unet, self).__init__()

        self.model =torch.load('/content/drive/MyDrive/Amrita Dessertation/Saved models/checkpath_efficientNet', map_location=torch.device('cpu'))

       # print(self.model.encoder.out_channels)
        self.level_set_head = nn.Sequential(
            nn.Linear(self.model.encoder.out_channels[-1], L_num),
            nn.ReLU(),
            nn.Linear(L_num,L_num),
            nn.ReLU(),
            nn.Linear(L_num, 1)
        )

    def forward(self, x):
        # U-Net segmentation prediction
        seg_pred = self.model(x)

        # Dual Consistency regression prediction
        features = self.model.encoder(x)[-1]
        #print(features)
        features = features.view(features.size(0), features.size(1), -1)
        #print(features)
        features = features.mean(dim=2)
        #print(features)
        level_set_pred = self.level_set_head(features)

        return seg_pred, level_set_pred







# Calculate the Dual-Task-Consistency Loss (LDT C)


model = Dual_Unet().to(device)

import torch
import numpy as np
from skimage.segmentation import find_boundaries

def generate_lsf_ground_truth(mask):
    binary_mask = torch.where(mask > 0, torch.tensor(1), torch.tensor(0))
    binary_mask_np = binary_mask.cpu().numpy()

    batch_size = mask.shape[0]  # Get the batch size
    lsf_ground_truth = torch.zeros_like(mask, dtype=torch.float32)

    for i in range(batch_size):
        boundary = find_boundaries(binary_mask_np[i], mode='inner').astype(np.uint8)
        lsf_ground_truth[i, boundary == 1] = 0
        lsf_ground_truth[i, binary_mask[i] == 1] = -torch.abs(lsf_ground_truth[i, binary_mask[i] == 1])
        lsf_ground_truth[i, binary_mask[i] == 0] = torch.abs(lsf_ground_truth[i, binary_mask[i] == 0])

    min_value = torch.min(lsf_ground_truth)
    max_value = torch.max(lsf_ground_truth)

    if min_value != float('-inf') and max_value != 0:
        lsf_ground_truth = (lsf_ground_truth - min_value) / (max_value - min_value) * 2 - 1
    else:
        lsf_ground_truth = torch.zeros_like(mask)

    return lsf_ground_truth

import matplotlib.image as mpimg
import torch
import numpy as np

mask_path = '/content/masks/148494967-148494986-001-d.bmp'
mask = mpimg.imread(mask_path)

mask = torch.from_numpy(np.array(mask)).float().unsqueeze(0).unsqueeze(0).to(device)  # Assuming 'device' is the GPU device

lsf_ground_truth = generate_lsf_ground_truth(mask)
print(lsf_ground_truth)

def level_set_to_seg(level_set_pred):
    k = 1.0  # Adjust this parameter to control the steepness of the sigmoid
    seg_map = 1 / (1 + torch.exp(-k * level_set_pred))
    return seg_map

class DualTaskConsistencyLoss(nn.Module):
    def __init__(self, k=1.0):
        super(DualTaskConsistencyLoss, self).__init__()
        self.k = k

    def forward(self, seg_pred, level_set_pred):
        seg_pred_transformed = self.inverse_transform(level_set_pred, seg_pred.size()[2:])
        consistency_loss = torch.mean(torch.pow(seg_pred - seg_pred_transformed, 2))
        return consistency_loss

    def inverse_transform(self, level_set_pred, size):
        upsampled_level_set_pred = F.interpolate(level_set_pred, size=size, mode='bilinear', align_corners=False)
        transformed_level_set_pred = torch.sigmoid(self.k * upsampled_level_set_pred)
        return transformed_level_set_pred

dual_consistency_loss = DualTaskConsistencyLoss(k=1)

metrics = [
    smp.utils.metrics.IoU(threshold = 0.5),
    smp.utils.metrics.Accuracy(threshold = 0.5),
]

optimizer = torch.optim.Adam([
    dict(params=model.parameters(), lr=0.0001),
])

def dice_Loss(inputs, targets, smooth=1e-6):
    intersection = torch.sum(inputs * targets)
    union = torch.sum(inputs) + torch.sum(targets)
    dice = (2 * intersection + smooth) / (union + smooth)
    loss = 1 - dice
    return loss

def llsf_loss(predictions, ground_truth):
    # Move ground_truth to the same device as predictions
    ground_truth = ground_truth.to(predictions.device)

    # Reshape the ground_truth tensor to match the shape of predictions
    ground_truth_reshaped = ground_truth.view(-1, 1, 1, 1)

    # Calculate the L2 loss between predictions and ground_truth_reshaped
    llsf_loss = torch.mean(torch.pow(predictions - ground_truth_reshaped, 2))

    return llsf_loss

import math

train_losses = []
valid_losses = []

train_ious = []
valid_ious = []
n_epochs = 30

max_score = 0
tmax=n_epochs * len(data_dictm['train_multi'])
for epoch in range(n_epochs):
    train_loss = 0.0
    valid_loss = 0.0
    train_iou = 0.0
    valid_iou = 0.0

    model.train()  # Set the model to train mode
    for t, (batch_lab, batch_ulab) in enumerate(zip(data_dicts['train'], data_dictm['train_multi']), 1):
        images_l, targets_l = batch_lab
        images_u = batch_ulab
        images_l = images_l.to(device)
        targets_l = targets_l.to(device)
        images_u = batch_ulab.to(device)


        optimizer.zero_grad()
        lsf_gt = generate_lsf_ground_truth(targets_l)
        seg_pred, level_set_pred = model(images_l)
        #print(lsf_gt.shape)
        #print(level_set_pred.shape)
        dice_loss = dice_Loss(seg_pred, targets_l)  #L_Dice
        L_lsf = llsf_loss(level_set_pred,lsf_gt)      #L_lsf
      # print(L_lsf)
        lev_set_map = level_set_to_seg(level_set_pred)
      #  print(level_set_pred.shape)
       # print(seg_pred.shape)
        lev_set_map = lev_set_map.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
        lev_set_map = lev_set_map.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
        lev_set_map = F.interpolate(lev_set_map, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)


        # Calculate the consistency loss
        consistency_loss_l = dual_consistency_loss(seg_pred, lev_set_map) # L_DTC
        seg_pred_u, level_set_pred_u = model(images_u)
        lev_set_map_u = level_set_to_seg(level_set_pred_u)
      #  print(level_set_pred.shape)
       # print(seg_pred.shape)
        lev_set_map_u = lev_set_map_u.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
        lev_set_map_u = lev_set_map_u.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
        lev_set_map_u = F.interpolate(lev_set_map_u, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)


        # Calculate the consistency loss
        consistency_loss_u = dual_consistency_loss(seg_pred_u, lev_set_map_u) # L_DTC



        lambda_d = math.exp(-5 * (1 - t / tmax)**2)
        #loss_total = dice_loss + L_lsf + lambda_d * consistency_loss
        loss_total = dice_loss  + lambda_d * consistency_loss_l +lambda_d * consistency_loss_u
     #   print(lambda_d)
        loss_total.backward()
        optimizer.step()

        train_loss += loss_total.item() * images_u.size(0)
        seg_pred_flat = seg_pred.argmax(dim=1).flatten().cpu().detach().round()
        lev_set_map_flat = lev_set_map.flatten().cpu().detach().round()
        train_iou += 1-dice_loss
    train_loss /= len(data_dictm['train_multi'].dataset)
    train_iou /= len(data_dictm['train_multi'].dataset)
    train_losses.append(train_loss)
    train_ious.append(train_iou)
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for t, (batch_lab, batch_ulab) in enumerate(zip(data_dicts['val'], data_dictm['val_multi']), 1):
            images_l, targets_l = batch_lab
            images_u = batch_ulab
            images_l = images_l.to(device)
            targets_l = targets_l.to(device)
            images_u = batch_ulab.to(device)
            lsf_gt = generate_lsf_ground_truth(targets_l)
            seg_pred, level_set_pred = model(images_l)
        #print(lsf_gt.shape)
        #print(level_set_pred.shape)
            dice_loss = dice_Loss(seg_pred, targets_l)  #L_Dice
            L_lsf = llsf_loss(level_set_pred,lsf_gt)      #L_lsf
            lev_set_map = level_set_to_seg(level_set_pred)
      #  print(level_set_pred.shape)
       # print(seg_pred.shape)
            lev_set_map = lev_set_map.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
            lev_set_map = lev_set_map.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
            lev_set_map = F.interpolate(lev_set_map, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)


        # Calculate the consistency loss
            consistency_loss_l = dual_consistency_loss(seg_pred, lev_set_map) # L_DTC
            seg_pred_u, level_set_pred_u = model(images_u)
            lev_set_map_u = level_set_to_seg(level_set_pred_u)
      #  print(level_set_pred.shape)
       # print(seg_pred.shape)
            lev_set_map_u = lev_set_map_u.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
            lev_set_map_u = lev_set_map_u.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
            lev_set_map_u = F.interpolate(lev_set_map_u, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)


        # Calculate the consistency loss
            consistency_loss_u = dual_consistency_loss(seg_pred_u, lev_set_map_u) # L_DTC



            lambda_d = math.exp(-5 * (1 - t / tmax)**2)
        #loss_total = dice_loss + L_lsf + lambda_d * consistency_loss
            loss_total = dice_loss  + lambda_d * consistency_loss_l +lambda_d * consistency_loss_u


            valid_loss += loss_total.item() * images_u.size(0)

            seg_pred_flat = seg_pred.argmax(dim=1).flatten().cpu().detach().round()
            lev_set_map_flat = lev_set_map.flatten().cpu().detach().round()
            valid_iou += 1-dice_loss



    # Calculate average validation loss
    valid_loss /= len(data_dictm['val_multi'].dataset)
    valid_iou /= len(data_dictm['val_multi'].dataset)
    valid_losses.append(valid_loss)
    valid_ious.append(valid_iou)


    # Check if the current epoch has the best validation score
    if valid_iou > max_score:
     #   max_score = valid_iou
       torch.save(model.state_dict(), '/content/drive/MyDrive/Amrita Dessertation/Phase -2/semi_final')
       print('Model saved!')

    # Adjust learning rate if necessary
    if epoch == 25:
        optimizer.param_groups[0]['lr'] = 1e-5

    # Print epoch statistics
    #print(f"Epoch: {epoch+1}\tTrain Loss: {train_loss:.4f}\tTrain IoU :{train_iou:.4f}\tValid Loss: {valid_loss:.4f}\tValid IoU: {valid_iou:.4f}")
    print(f"Epoch: {epoch+1}\tTrain Loss: {train_loss:.4f}\tValid Loss: {valid_loss:.4f}")



plt.title("IoU score vs. Epochs")

plt.xlabel("Epochs")
plt.ylabel("IoU score")
plt.plot(range(1, n_epochs+1), [t.item() for t in train_ious], label="Train")
plt.plot(range(1, n_epochs+1), [t.item() for t in valid_ious], label="Validation")




plt.legend(["training_set IoU Score","Valodation_set IoU Score"])

plt.show()

plt.title("Loss vs. Epochs")

plt.xlabel("Epochs")
plt.ylabel("Loss")

plt.plot(range(1,n_epochs+1),train_losses,label="Train")
plt.plot(range(1,n_epochs+1),valid_losses,label="Validation")

plt.legend(["Training_set Loss","Validation_set Loss"])

plt.show()

train_losses = []
valid_losses = []

train_ious = []
valid_ious = []
n_epochs = 30

max_score = 0

for epoch in range(n_epochs):
    train_loss = 0.0
    valid_loss = 0.0
    train_iou = 0.0
    valid_iou = 0.0

    model.train()  # Set the model to train mode
    for data in data_dictm['train_multi']:
        inputs = data.to(device)

        optimizer.zero_grad()
        seg_pred, level_set_pred = model(inputs)

        lev_set_map = level_set_to_seg(level_set_pred)
      #  print(level_set_pred.shape)
       # print(seg_pred.shape)
        lev_set_map = lev_set_map.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
        lev_set_map = lev_set_map.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
        lev_set_map = F.interpolate(lev_set_map, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)


        # Calculate the consistency loss
        consistency_loss = dual_consistency_loss(seg_pred, lev_set_map)

        consistency_loss.backward()
        optimizer.step()

        train_loss += consistency_loss.item() * inputs.size(0)
        seg_pred_flat = seg_pred.argmax(dim=1).flatten().cpu().detach().round()
        lev_set_map_flat = lev_set_map.flatten().cpu().detach().round()
        train_iou += 1- train_loss
    train_loss /= len(data_dictm['train_multi'].dataset)
    train_iou /= len(data_dictm['train_multi'].dataset)
    train_losses.append(train_loss)
    train_ious.append(train_iou)
    model.eval()  # Set the model to evaluation mode
    with torch.no_grad():
        for data in data_dictm['val_multi']:
            inputs = data.to(device)

            seg_pred, level_set_pred = model(inputs)
           # print(level_set_pred.shape)
           # print(seg_pred.shape)
            lev_set_map = level_set_to_seg(level_set_pred)
            lev_set_map = lev_set_map.view(-1, 1, 1, 1)  # Reshape to [4, 1, 1, 1]
            lev_set_map = lev_set_map.expand(-1, 1, 256, 256)  # Expand dimensions to [4, 1, 256, 256]
            lev_set_map = F.interpolate(lev_set_map, size=seg_pred.shape[2:], mode='bilinear', align_corners=False)

            # Calculate the consistency loss
            consistency_loss = dual_consistency_loss(seg_pred, lev_set_map)

            valid_loss += consistency_loss.item() * inputs.size(0)

            seg_pred_flat = seg_pred.argmax(dim=1).flatten().cpu().detach().round()
            lev_set_map_flat = lev_set_map.flatten().cpu().detach().round()
            valid_iou += 1-valid_loss



    # Calculate average validation loss
    valid_loss /= len(data_dictm['val_multi'].dataset)
    valid_iou /= len(data_dictm['val_multi'].dataset)
    valid_losses.append(valid_loss)
    valid_ious.append(valid_iou)


    # Check if the current epoch has the best validation score
    #if valid_iou > max_score:
     #   max_score = valid_iou
    torch.save(model.state_dict(), '/content/drive/MyDrive/Amrita Dessertation/Phase -2/unsupervised_final')
    print('Model saved!')

    # Adjust learning rate if necessary
    if epoch == 25:
        optimizer.param_groups[0]['lr'] = 1e-5

    # Print epoch statistics
    #print(f"Epoch: {epoch+1}\tTrain Loss: {train_loss:.4f}\tTrain IoU :{train_iou:.4f}\tValid Loss: {valid_loss:.4f}\tValid IoU: {valid_iou:.4f}")
    print(f"Epoch: {epoch+1}\tTrain Loss: {train_loss:.4f}\tValid Loss: {valid_loss:.4f}")

plt.title("IoU score vs. Epochs")

plt.xlabel("Epochs")
plt.ylabel("IoU score")

plt.plot(range(1,n_epochs+1),train_ious,label="Train")
plt.plot(range(1,n_epochs+1),valid_ious,label="Validation")

plt.legend(["training_set IoU Score","Validation_set IoU Score"])

plt.show()

plt.title("Loss vs. Epochs")

plt.xlabel("Epochs")
plt.ylabel("Loss")

plt.plot(range(1,n_epochs+1),train_losses,label="Train")
plt.plot(range(1,n_epochs+1),valid_losses,label="Validation")

plt.legend(["Training_set Loss","Validation_set Loss"])

plt.show()

# load best saved checkpoint
#best_model = torch.load('/content/gdrive/MyDrive/Final Project/checkpath.pth')
best_model = torch.load('/content/drive/MyDrive/Amrita Dessertation/Phase -2/unsupervised_final', map_location=torch.device('cpu'))
# create test dataset

test_data = test_set_multi

# evaluate model on test set
test_epoch = smp.utils.train.ValidEpoch(
    model=best_model,
    loss=loss,
    metrics=metrics,
    device=DEVICE,
)

logs = test_epoch.run(test_data)

model.load_state_dict(torch.load('/content/drive/MyDrive/Amrita Dessertation/Phase -2/unsupervised_final', map_location=torch.device('cpu')))

# Move the model to the desired device
model = model.to(device)

# Set the model to evaluation mode
model.eval()
test_data = torch.utils.data.DataLoader(test_set_multi)

for i in range(len(test_set_multi)):
    image = test_set_multi[i]
    image = image.unsqueeze(0).to(device)

    # Perform inference
    with torch.no_grad():
        seg_pred = model(image)

    seg_pred = seg_pred[0]  # Access the tensor containing the predictions

# Convert the tensor prediction to numpy array
    seg_pred_np = torch.argmax(seg_pred, dim=1).squeeze().cpu().numpy()

    # Visualize the original image and predicted segmentation
    fig, axes = plt.subplots(1, 2, figsize=(10, 5))
    axes[0].imshow(image.squeeze().cpu().numpy().transpose(1, 2, 0))
    axes[0].set_title("Original Image")
    axes[1].imshow(seg_pred_np, cmap='gray')
    axes[1].set_title("Segmentation Prediction")
    plt.show()

"""### XXX"""

import gc

n_epochs = 10
optimizer = optim.Adam(model.parameters(), lr=0.001)
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(device)



for epoch in range(1):
    print('\nEpoch: {}'.format(epoch))
    total_loss = 0.0
    total_batches = 0
   # c=0
    for images_u in data_dictm['train_multi']:
        images_u = images_u

        optimizer.zero_grad()
        #images_u = batch_ulab[:, :3, :, :].to(device)
        seg_pred, level_pred = model.forward(images_u)


        level_pred_map = level_set_to_seg(level_pred)
        #level_pred_reshaped = level_pred.view(level_pred.size(0), level_pred.size(1), 1, 1)
       # level_pred_r = F.interpolate(level_pred_reshaped, size=(256, 256), mode='bilinear', align_corners=False)

        loss_consistency = dual_task_consistency_loss(seg_pred, level_pred_map)

        total_loss += loss_consistency
        total_batches += 1

        gc.collect()
        torch.cuda.empty_cache()


    avg_loss = total_loss / total_batches
    print('Average Loss: {:.4f}'.format(avg_loss))

train_epoch = smp.utils.train.TrainEpoch(
    model,
    loss=dual_consistency_loss,
    metrics=metrics,
    optimizer=optimizer,
    device=device,
    # verbose=True,
)

valid_epoch = smp.utils.train.ValidEpoch(
    model,
    loss=dual_consistency_loss,
    metrics=metrics,
    device=device,
    # verbose=True,
)

vloss = []
tloss = []

vdloss = []
tdloss = []

n_epochs = 60

max_score = 0

for i in range(0,n_epochs):

    print('\nEpoch: {}'.format(i))
    train_logs = train_epoch.run(data_dictm['train_multi'])
    valid_logs = valid_epoch.run(data_dictm['val_multi'])

    # do something (save model, change lr, etc.)
    if max_score < valid_logs['iou_score']:
        max_score = valid_logs['iou_score']
        torch.save(model, '/content/gdrive/MyDrive/Amrita Dessertation/Saved models/aug_trialfinal1')
        print('Model saved!')

    if i == 25:
        optimizer.param_groups[0]['lr'] = 1e-5

    tloss.append(train_logs['iou_score'])
    vloss.append(valid_logs['iou_score'])

    tdloss.append(train_logs['dice_loss'])
    vdloss.append(valid_logs['dice_loss'])

print(train_losses)

model = torch.load('/content/drive/MyDrive/Amrita Dessertation/Saved models/checkpath_efficientNet', map_location=device)
# create test dataset

test_data = torch.utils.data.DataLoader(test_set_multi)

model = model.to(device)
model.eval()

# Iterate over the multi-cell dataset and perform inference
for images in test_data:
    images = images.to(device)

    # Perform inference
    with torch.no_grad():
        seg_pred, level_set_pred = model(images)



class Dual_Unet(nn.Module):
    def __init__(self, encoder_name='efficientnet-b0', encoder_weights='imagenet'):
        super(Dual_Unet, self).__init__()

        self.model = smp.Unet(
            encoder_name=encoder_name,
            encoder_weights=encoder_weights,
            classes=3,
            activation='softmax'
        )
        print(self.model.encoder.out_channels)

        self.level_set_head = nn.Sequential(
            nn.Linear(self.model.encoder.out_channels[-1], NUM_LEVEL_SET_UNITS),
            nn.ReLU(),
            nn.Linear(NUM_LEVEL_SET_UNITS, NUM_LEVEL_SET_UNITS),
            nn.ReLU(),
            nn.Linear(NUM_LEVEL_SET_UNITS, 1)
        )

    def forward(self, x):
        # U-Net segmentation prediction
        seg_pred = self.model(x)

        # Dual Consistency regression prediction
        features = self.model.encoder(x)[-1]  # Access the last element of the list
        features = features.view(features.size(0), features.size(1), -1)  # Reshape to (batch_size, channels, -1)
        features = features.mean(dim=2)  # Average along the spatial dimensions
        level_set_pred = self.level_set_head(features)

        return seg_pred, level_set_pred

model = Dual_Unet()

print(model)

segmentation_criterion = nn.CrossEntropyLoss()
level_set_criterion = nn.MSELoss()

optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

import torch
import numpy as np
from scipy.ndimage import distance_transform_edt

def generate_lsf_ground_truth(mask):
    # Convert mask to binary tensor
    binary_mask = torch.where(mask > 0, torch.tensor(1), torch.tensor(0))

    # Convert binary mask to NumPy array
    binary_mask_np = binary_mask.numpy()

    # Compute Euclidean distance transforms using scipy.ndimage
    inner_dist = distance_transform_edt(1 - binary_mask_np)
    outer_dist = distance_transform_edt(binary_mask_np)

    # Convert distance transforms to torch.Tensor of type torch.FloatTensor
    inner_dist = torch.from_numpy(inner_dist).float()
    outer_dist = torch.from_numpy(outer_dist).float()

    # Convert mask to torch.FloatTensor
    mask = mask.float()

    # Compute LSF ground truth
    lsf_ground_truth = torch.zeros_like(mask)
    lsf_ground_truth[mask == 0] = -float('inf')
    lsf_ground_truth[mask > 0] = inner_dist[mask > 0] ** 2
    lsf_ground_truth[mask == 0] = 0
    lsf_ground_truth[mask < 0] = outer_dist[mask < 0] ** 2
    lsf_ground_truth = torch.where(torch.isinf(lsf_ground_truth), torch.finfo(torch.float32).max, lsf_ground_truth)

    return lsf_ground_truth

def dual_task_consistency_loss(segmentation_pred, level_set_pred):
    # Compute the consistency loss between segmentation and level set predictions
    consistency_loss = torch.mean(torch.abs(segmentation_pred - level_set_pred))
    return consistency_loss

n_epochs=10

# train model for 20 epochs
vloss = []
tloss = []

vdloss = []
tdloss = []

n_epochs = 60

max_score = 0

for i in range(0,n_epochs):

    print('\nEpoch: {}'.format(i))
    train_logs = train_epoch.run(data_dict['train'])
    valid_logs = valid_epoch.run(data_dict['val'])

    # do something (save model, change lr, etc.)
    if max_score < valid_logs['iou_score']:
        max_score = valid_logs['iou_score']
        torch.save(model, '/content/gdrive/MyDrive/Amrita Dessertation/Saved models/eff_without encoder_preprocess')
        print('Model saved!')

    if i == 25:
        optimizer.param_groups[0]['lr'] = 1e-5

    tloss.append(train_logs['iou_score'])
    vloss.append(valid_logs['iou_score'])

    tdloss.append(train_logs['dice_loss'])
    vdloss.append(valid_logs['dice_loss'])

model= Dual_Unet()

import torch.nn.functional as F

torch.autograd.set_detect_anomaly(True)
for i in range(n_epochs):
    print('\nEpoch: {}'.format(i))
    total_loss = 0.0
    total_batches = 0

    for batch_lab, batch_ulab in zip(data_dict['train'], data_dictm['train_multi']):
        images_l, targets_l = batch_lab
        images_u = batch_ulab
        if torch.cuda.is_available():
            images_l, labels = images.cuda(), labels.cuda()

        #print(images_l.size())

        lsf_ground_truth = generate_lsf_ground_truth(targets_l)
        seg_pred, level_pred = model(images_l)

       # print(level_pred.size())
       # print(lsf_ground_truth.size())
        loss_seg = segmentation_criterion(seg_pred, targets_l)

        level_pred_reshaped = level_pred.view(level_pred.size(0), level_pred.size(1), 1, 1)
        level_pred_r = F.interpolate(level_pred_reshaped, size=(256, 256), mode='bilinear', align_corners=False)
        loss_level_set = level_set_criterion(level_pred_r, lsf_ground_truth)
        loss_consistency = dual_task_consistency_loss(seg_pred, level_pred_r)

        total_loss += loss_seg + loss_level_set + 0.1 * loss_consistency
        total_batches += 1

        optimizer.zero_grad()
        total_loss.backward(retain_graph=True)
        optimizer.step()

    avg_loss = total_loss / total_batches
    print('Average Loss: {:.4f}'.format(avg_loss.item()))

# Training loop
for epoch in range(num_epochs):
    for batch_l, batch_u in zip(data_loader_labeled, data_loader_unlabeled):
        images_l, targets_l = batch_l
        images_u, _ = batch_u


        # Generate LSF ground truth T(yi) according to Equation 1 for labeled data
        lsf_ground_truth = generate_lsf_ground_truth(targets_l)

        # Compute dual-task predictions
        segmentation_pred, level_set_pred = model(images_l)

        # Apply task transform layer T^(-1)(f2(xi)) according to Equation 2

        # Compute losses
        loss_segmentation = segmentation_criterion(segmentation_pred, targets_l)
        loss_level_set = level_set_criterion(level_set_pred, lsf_ground_truth)
        loss_consistency = dual_task_consistency_loss(segmentation_pred, level_set_pred)

        # Compute total loss
        total_loss = loss_segmentation + loss_level_set + lambda_consistency * loss_consistency

        # Backpropagation and optimization steps
        optimizer.zero_grad()
        total_loss.backward()
        optimizer.step()

model = torch.load('/content/drive/MyDrive/Amrita Dessertation/Saved models/checkpath_efficientNet')